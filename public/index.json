[{"categories":["Data Analysis","Interactive","Visualisation","Linear Regression","Machine Learning"],"content":"A deep dive into the Coronavirus data","date":"2020-03-28","objectID":"/covid19-interactive-analysis/","tags":["blogging","data science","machine learning","covid-19"],"title":"COVID-19: An Interactive Analysis","uri":"/covid19-interactive-analysis/"},{"categories":["Data Analysis","Interactive","Visualisation","Linear Regression","Machine Learning"],"content":"A new virus has plauged us and caused turmoil in the world. This virus has caused shut-downs, lockdowns and in the worst-case will unfortunately cause deaths. It is our job as responsible citizens to do our best to stop it from further spreading, and what better way than for us data scientists to dive deep into the data (of course while wearing masks and keeping the safe distance). So lets dive in. ","date":"2020-03-28","objectID":"/covid19-interactive-analysis/:0:0","tags":["blogging","data science","machine learning","covid-19"],"title":"COVID-19: An Interactive Analysis","uri":"/covid19-interactive-analysis/"},{"categories":["Data Analysis","Interactive","Visualisation","Linear Regression","Machine Learning"],"content":"Introduction \rThe SARS-CoV-2 virus causes Coronavirus Disease (COVID-19), an infectious respiratory disease. The majority of those infected with the virus will have mild to moderate respiratory symptoms and will recover without the need for medical attention. Some, on the other hand, will become critically unwell and require medical assistance. Serious sickness is more likely to strike the elderly and those with underlying medical disorders such as cardiovascular disease, diabetes, chronic respiratory disease, or cancer. COVID-19 can make anyone sick and cause them to get very ill or die at any age. But how does the virus spread? Coughing, sneezing, and talking are the most common ways for the virus to spread through small droplets. Although the droplets are not normally airborne, persons who are in close proximity to them may inhale them and become infected. By contacting a contaminated surface and subsequently touching their face, people can become sick. Aerosols that can stay suspended in the air for prolonged periods of time in confined places may also be a source of transmission. It is most contagious in the first three days after symptoms develop, but it can also spread before symptoms appear and from asymptomatic people. Which strongly explains the value of wearing well fitted masks and as further illustrated in the GIF below. \rGIF Source: Infrared video shows the risks of airborne coronavirus spread | Visual Forensics\r\rSo if we ask ourselves how can we prevent COVID-19 from spreading, we‚Äôll find these main precautions to take: Keep a safe distance away from somebody coughing or sneezing. When physical separation isn‚Äôt possible, wear a mask. Seek medical help if you have a fever, cough, or difficulty breathing. If you‚Äôre sick, stay at home. Hands should be washed frequently. Use soap and water or an alcohol-based hand rub to clean your hands. Keep your hands away from your eyes, nose, and mouth. When you cough or sneeze, cover your nose and mouth with your bent elbow or a tissue. How is the virus detected? Real-time reverse transcription polymerase chain reaction (rRT-PCR) from a nasopharyngeal swab is the usual method of diagnosis. Although chest CT imaging may be useful for diagnosis in patients with a strong suspicion of infection based on symptoms and risk factors, it is not recommended for routine use¬†screening (Wikipedia), which may present an option to utilize computer vision for example by using convolutional neural networks to detect the virus in CT image scans, we‚Äôll explore this another time in a different article. ","date":"2020-03-28","objectID":"/covid19-interactive-analysis/:1:0","tags":["blogging","data science","machine learning","covid-19"],"title":"COVID-19: An Interactive Analysis","uri":"/covid19-interactive-analysis/"},{"categories":["Data Analysis","Interactive","Visualisation","Linear Regression","Machine Learning"],"content":"Diving Deep into the data \rAll these previous precautions mentioned are of tremendous importance to stop the virus from further spreading but these won‚Äôt allow us to study more concisely where it spreads, why it spreads in areas more than others and how we can flatten the curve, as they are proactive measures, we are trying to analyze the historical (even though the virus is still fairly young) data through reactive measures of data analysis and machine learning, of course proactive (unsupervised learning) measures can also be deployed. That‚Äôs where data analysis comes into play. I dug deep into the data and all the accompaying code in this article can be found in this Github repository: Covid-19-Analysis. The data is provided by John Hopkins University, it is available in their Github repo. The data is updated daily. I will also be using data from Our World In Data and acaps for further data exploration. Lets start by first importing all required libraries. And setting some default settings import pandas as pd import numpy as np import itertools import os import warnings from itertools import tee warnings.filterwarnings('ignore') import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots import plotly.io as pio import plotly.offline as py import ipywidgets as widgets py.init_notebook_mode() pio.renderers.default = \"notebook\" pd.options.plotting.backend = \"plotly\" pd.options.display.max_rows = 20 from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error from sklearn.model_selection import train_test_split #from countryinfo import CountryInfo #Populations not up to date import pypopulation import pycountry I‚Äôll be using plotly for interactive plotting. ipywidgets for interactive user data filtering and selection. Pandas and numpy for data manipulation. scikit-learn for machine learning and further corresponding libraries for other utilities. We‚Äôll then load the datasets using Pandas straight from the source URL, because in this way everytime the code is ran it will have the most up to date data, rather than a downloaded Excel or CSV file. # Only use the columns we need cols = ['iso_code','continent','location','date' ,'total_cases','new_cases','total_deaths', 'new_deaths','total_cases_per_million', 'new_cases_per_million','total_deaths_per_million', 'new_deaths_per_million','new_tests', 'total_tests','total_tests_per_thousand', 'new_tests_per_thousand','new_tests_smoothed', 'new_tests_smoothed_per_thousand', 'tests_units','stringency_index', 'population','population_density', 'median_age','aged_65_older','aged_70_older', 'gdp_per_capita','extreme_poverty', 'cardiovasc_death_rate','diabetes_prevalence', 'female_smokers','male_smokers', 'handwashing_facilities', 'hospital_beds_per_thousand','life_expectancy'] df_stats = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv',delimiter=',') df_stats = df_stats[cols] df_measures = pd.read_excel('https://www.acaps.org/sites/acaps/files/resources/files/acaps_covid19_government_measures_dataset_0.xlsx', header=0,sheet_name='Dataset') df_confirmed = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv') df_deaths = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv') df_recoveries = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv') Shape for Statistics DataFrame: (167554, 34)\rShape for Measures DataFrame: (23923, 18)\rShape for Confirmed DataFrame: (284, 783)\rShape for Deaths DataFrame: (284, 783)\rShape for Recovories DataFrame: (269, 783)\r After loading the data and printing","date":"2020-03-28","objectID":"/covid19-interactive-analysis/:2:0","tags":["blogging","data science","machine learning","covid-19"],"title":"COVID-19: An Interactive Analysis","uri":"/covid19-interactive-analysis/"},{"categories":["Linear Regression","Tutorial","Machine Learning"],"content":"All you need to know","date":"2020-03-10","objectID":"/linear-regression-all-you-need-to-know/","tags":["blogging","data science","machine learning","linear regression","python"],"title":"Linear Regression","uri":"/linear-regression-all-you-need-to-know/"},{"categories":["Linear Regression","Tutorial","Machine Learning"],"content":"What a better way to start my blogging journey, than with one of the most fundamental statistical learning techniques, Linear Regression. A straightforward method for supervised learning - supervised learning is the process of training a model on data where the outcome is known before applying it to data where the outcome is unknown -, learning linear regression also helps to understand the overall process of what supervised learning looks like. Linear regression, in particular, is a powerful tool for predicting a quantitative response. It‚Äôs been around for a while and is the subject of a slew of textbooks. Linear Regression is part of the Generalized Linear Models family (GLM for short). Despite the fact that it may appear tedious in comparison to some of the more current statistical learning approaches, linear regression remains an effective and extensively used statistical learning method. It also provides as a useful starting point for emerging approaches. Many fancy statistical learning methods can be thought of as extensions or generalizations of linear regression. As a result, the necessity of mastering linear regression before moving on to more advanced learning approaches cannot be emphasized. The response to the question ‚ÄúIs the variable $X$ associated with a variable $Y$, and if so, what is the relationship, and can we use it to predict Y?‚Äù is perhaps the most prevalent goal in statistics and linear regression tries to answer that, it does this based on linear relationships between the independent ($X$) and dependent ($Y$) variables. Simple linear regression creates a model of the relationship between the magnitudes of two variables‚Äîfor example, as X increases, Y increases as well. Alternatively, when X increases, Y decreases. I‚Äôve used the term simple here because we are only talking about one variable X, but instead of one variable X, we can of course use multiple predictor variables $ X_{1}‚Ä¶X_{n} $, which is often termed Multiple Linear Regression or just simply Linear Regression. I assure you that I have a multitude of examples and explanations that go through all of the finer points of linear regression. But first, let‚Äôs go through the fundamental concepts. ","date":"2020-03-10","objectID":"/linear-regression-all-you-need-to-know/:0:0","tags":["blogging","data science","machine learning","linear regression","python"],"title":"Linear Regression","uri":"/linear-regression-all-you-need-to-know/"},{"categories":["Linear Regression","Tutorial","Machine Learning"],"content":"The fundamental concepts behind linear regression The first step in linear regression is to fit a line to the data using least squares The second step is to compute R2 Finally, compute the p-value for the computed R squared in the previous step The first concept we are going to tackle is fitting a line to the data, what exactly does that mean and how can we do it? To effectively explain that we are going to need some data. I will be using the fish market data from Kaggle which is available here. The dataset contains sales records for seven common fish species seen in fish markets. Some of the features (columns) in dataset are: Species, Weight, Different Lengths, Height and Width. We‚Äôll be using it to estimate the weight of the fish, as we are trying to explain linear regression we‚Äôll first use only one feature to predict the weight, the height (simple linear regression). We‚Äôll first load the dataset using pandas read_csv: df = pd.read_csv(\"data/Fish.csv\") All code in this article can be found in my Github in this repository Now back to our first concept, fitting a line to the data, what does that mean? Fitting a line to the data To clearly explain this concept let‚Äôs first get only one species of fish, in this example we‚Äôll use Perch: df_perch = df[df.Species == 'Perch'] We‚Äôll then simply create a scatter plot of Weight vs. Height. fig = px.scatter(df_perch, x='Height', y='Weight') fig.show() And then we‚Äôll plot a horizontal line across the data, at the average weight (our target variable). mean_val= df_perch.Weight.mean() fig = fig.add_hline(y = mean_val,row= None, col = None) fig.show() Then we calculate the difference between the actual values, the dots representing the weights in the figure, and the line (the mean value). This can be thought of as the distance between them (if you would like to think geometrically). These distances are also called the residuals. Here are the residuals plotted. for weight,height in zip(df_perch.Weight,df_perch.Height): fig.add_shape(type='line',x0=height,x1=height,y0=weight,y1=mean_val,line=dict(width=1,dash='dash',color='red')) fig.show() Then we‚Äôll square each distance (residual) from the line to the data and sum them all up. We square the residuals so that the negative and positive residuals don‚Äôt cancel each other out. residuals_squared = (df_perch.Weight - mean_val)**2 residuals_squared.sum() We get a value of 6646094.253571428. That‚Äôs a very large number, but we don‚Äôt want that we want it to be as low as possible, and how can we do that? That‚Äôs the next step. Third we rotate and/or shift the line a little bit to be able to decrease this sum of squared distances or residuals to its lowest possible value, that is where the name least squares (or ordinary least squares) comes from. To do that we need to understand what exactly a line is mathematically. You probably remember from linear algebra that a line is just this equation $ y = mx +b $, where $m$ is the slope of the line $b$ is the y-intercept, $x$ is the x-axis value and $y$ is the y-axis value. Well to rotate the line we need to iteratively - or mathematically - adjust the y-intercept ($b$) and the slope ($m$) so that the sum of the squared residuals - the error - is the lowest. In linear regression the equation is usually expressed in a different way like so: $\\hat{y} = {\\alpha} + {\\beta}x$, where $ {\\alpha} $ is the y-intercept (sometimes also written as $ {\\beta}_0 $) and $ {\\beta} $ is the slope. In simple linear regression there is only one independent variable - feature - and therefore only one slope - or coefficient -, but in many cases we have many different features that we would like to use in our equation so for every feature $X_n$ added we add its coefficient $ {\\beta}_n$, to be estimated with the y-intercept. In general, such a relationship may not hold perfectly for the mostly unobserved population of values of the feature and target variables; these unobserved variations from the above equation are referred to as ran","date":"2020-03-10","objectID":"/linear-regression-all-you-need-to-know/:0:1","tags":["blogging","data science","machine learning","linear regression","python"],"title":"Linear Regression","uri":"/linear-regression-all-you-need-to-know/"},{"categories":["General"],"content":"The beginning of sharing my journey publicly","date":"2020-03-04","objectID":"/why-i-started-blogging/","tags":["blogging","data science","machine learning"],"title":"Why I started Blogging","uri":"/why-i-started-blogging/"},{"categories":["General"],"content":"I‚Äôve been studying machine learning, data analysis and all-stuff data for almost a year now. It‚Äôs been an exciting journey, I got to talk to interesting people, work on meaningful projects but most importantly learn, which is really my favourite activity. Plus, my passion for data and being able to use it in worthwhile ways, that will help others grew significantly. I already had an impulse to patterns, numbers, math and reasoning which definitely helped shape my ambition in learning machine learning, data analysis and pursuing a career in data science, not because it was labelled ‚ÄúThe Sexiest Job Of The 21st Century‚Äù by the Harvard Business Review back in 2012. After this small but growing journey, I decided to document it, in the form of blog posts. I worked on several projects so I will be talking about them and sharing them. I tried and still try to understand different algorithms and techniques in my own way, so I will write about them in my own way of thinking. I will also be working on new projects, and I‚Äôll try my best to share them as well. There are other several reasons why I decided to start my own blog, here are some of them: It can help others. Others may benefit from the way I went about several of my own projects and my own way of understanding different concepts. It helps me get feedback on my work. While my blog posts can help others, I am not perfect and I might have gotten something wrong and any feedback from others can help me better myself. It‚Äôs like a portfolio/resume of my work but even better! Because not only do I showcase my skills I can get directly contacted for job offers. I‚Äôve used my previous projects with my previous employers to land several jobs. It helps me learn. Organizing information always aids me in formulating my own thoughts. One of the ways to tell if I understand something is to explain it to someone, and I do that all the time with others, but now just only in blog posts. It can also help me get to know others and even collaborate. I‚Äôve made a summary on my previous work with the help of my projects, and notes. I‚Äôll be using this summary to help me write these blog posts and maybe even enhance/update my previous work, and I advise anyone in the field of data science to try and do the same thing, there‚Äôs really nothing negative that can come from it, it doesn‚Äôt have to be an article a day, or even an article a week, you can do it at your own pace and build it up slowly. I will be writing about: Projects I have worked on Data analysis and business intelligence: from data cleaning, pre-processing, visualization, interactivity, applied statistics and data analysis tools. Machine Learning Algorithms: Regression, Classification, Clustering, etc. Deep Learning: Backpropagation, Activation Functions, Computer Vision, Natural Language Processing, etc. And who knows there might be new topics I would like to write about, only time will tell üòâ. I hope I got your attention, and you are willing to embark with me in my journey. I am super excited to write, code and share all the work I have been working on and will be working on. You can of course subscribe to my feed and get all the latest blog posts as soon as they are uploaded to my website. I also write in Medium here: @arebi.mohamed59. All my other social links such as GitHub and LinkedIn are to be found at the bottom of this page. Thank you for reading and see you on soon in the next article. Stay Safe! ","date":"2020-03-04","objectID":"/why-i-started-blogging/:0:0","tags":["blogging","data science","machine learning"],"title":"Why I started Blogging","uri":"/why-i-started-blogging/"}]